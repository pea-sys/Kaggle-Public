{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Optiver Realized Volatility Prediction.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jhx-H4kJ9SRn"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "from joblib import Parallel, delayed\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy as sc\n",
        "from sklearn.model_selection import KFold\n",
        "import lightgbm as lgb\n",
        "import warnings\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('max_columns', 300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nc86Iy9T96J6"
      },
      "source": [
        "def seed_everything(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "seed_everything(707)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6ylovRc9_Qp"
      },
      "source": [
        "SMALL_F = 0.00000001\n",
        "TARGET_SCALE = 1000\n",
        "# data directory\n",
        "data_dir = '../input/optiver-realized-volatility-prediction/'\n",
        "\n",
        "def calc_wap1(df):\n",
        "    wap = (df['bid_price1'].values * df['ask_size1'].values + df['ask_price1'].values * df['bid_size1'].values) / (df['bid_size1'].values + df['ask_size1'].values)\n",
        "    return wap\n",
        "\n",
        "def calc_wap2(df):\n",
        "    wap = (df['bid_price2'].values * df['ask_size2'].values + df['ask_price2'].values * df['bid_size2'].values) / (df['bid_size2'].values + df['ask_size2'].values)\n",
        "    return wap\n",
        "\n",
        "def spread_ratio(series):\n",
        "    if len(series) > 3:\n",
        "        return ((series.nlargest(3).mean()) / (series.nsmallest(3).mean())) - 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "def largest_mean(series):\n",
        "    if len(series) > 5:\n",
        "        return series.nlargest(5).mean()\n",
        "    else:\n",
        "        return series.max()\n",
        "    \n",
        "def smallest_mean(series):\n",
        "    if len(series) > 5:\n",
        "        return series.nsmallest(5).mean()\n",
        "    else:\n",
        "        return series.min()\n",
        "\n",
        "def log_return(series):\n",
        "    return np.log(series).diff()\n",
        "\n",
        "def log_return_half(series):\n",
        "    return np.log(series).diff(int(len(series)/2))\n",
        "\n",
        "def realized_volatility(series):\n",
        "    return np.sum(series**2)\n",
        "\n",
        "def count_unique(series):\n",
        "    return len(np.unique(series))\n",
        "\n",
        "# Function to calculate the root mean squared percentage error\n",
        "def rmspe(y_true, y_pred):\n",
        "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
        "\n",
        "# Function to early stop with root mean squared percentage error\n",
        "def feval_rmspe(y_pred, lgb_train):\n",
        "    y_true = lgb_train.get_label()\n",
        "    y_pred[y_pred <= SMALL_F] = SMALL_F\n",
        "    return 'RMSPE', rmspe(np.sqrt(y_true), np.sqrt(y_pred)), False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e06IJG0o-SZy"
      },
      "source": [
        "# Function to read our base train and test set\n",
        "def read_train_test():\n",
        "    train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
        "    train['target'] = train['target'].values ** 2\n",
        "    train['target'] = train['target'].values * TARGET_SCALE\n",
        "    #https://www.kaggle.com/c/optiver-realized-volatility-prediction/discussion/271920\n",
        "    #外れ値除去 + 上場廃止 or 売買停止 考慮 \n",
        "    train = train.loc[~((train['stock_id'] == 31) & (train['time_id'] == 25504))]\n",
        "    train = train.loc[~((train['stock_id'] == 31) & (train['time_id'] == 27174))]\n",
        "    train = train.loc[~((train['stock_id'] == 81) & (train['time_id'] == 28319))]\n",
        "    train = train.loc[~((train['stock_id'] == 31) & (train['time_id'] == 1544))]\n",
        "    train = train.loc[~((train['stock_id'] == 27) & (train['time_id'] == 20551))]\n",
        "    # 株式廃止や取引停止に備える\n",
        "    drop_time = random.sample(list(train['time_id'].unique()),3)\n",
        "    for t in drop_time:\n",
        "        drop_stock = random.sample(list(train.loc[train['time_id'] == t]['stock_id'].unique()),3)\n",
        "        for s in drop_stock:\n",
        "            train = train.loc[~((train['stock_id'] == s) & (train['time_id'] == t))]\n",
        "    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n",
        "\n",
        "    print(f'Our training set has {train.shape[0]} rows')\n",
        "    return train, test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zaiihFz-ZOz"
      },
      "source": [
        "def book_preprocessor(file_path):\n",
        "    df = pd.read_parquet(file_path)\n",
        "    for c in ['bid_price1','ask_price1','bid_price2','ask_price2']:\n",
        "        df[c].fillna(1,inplace=True)\n",
        "    for c in ['bid_size1','ask_size1','bid_size2','ask_size2']:\n",
        "        df[c].fillna(0,inplace=True)\n",
        "        \n",
        "    df = df.sort_values(['time_id','seconds_in_bucket'])\n",
        "\n",
        "    df['wap1'] = calc_wap1(df)\n",
        "    df['wap2'] = calc_wap2(df)\n",
        "\n",
        "    for n in [1,2]:\n",
        "        df[f'log_return{n}'] = df.groupby(['time_id'])[f'wap{n}'].apply(log_return).fillna(0)\n",
        "        df[f'log_return{n}_half'] = df.groupby(['time_id'])[f'wap{n}'].apply(log_return_half).fillna(0)\n",
        "        df[f'diff_log_return{n}'] = df[f'log_return{n}'].diff().fillna(0)\n",
        "        df[f\"log_return{n}_minus\"] = 0\n",
        "        df.loc[df[f\"log_return{n}\"]<0,f\"log_return{n}_minus\"] = df[f\"log_return{n}\"]\n",
        "        df[f\"log_return{n}_plus\"] = 0\n",
        "        df.loc[df[f\"log_return{n}\"]>0,f\"log_return{n}_plus\"] = df[f\"log_return{n}\"]\n",
        "    \n",
        "    df['ask_volume1'] = df['ask_price1'].values * df['ask_size1'].values\n",
        "    df['ask_volume2'] = df['ask_price2'].values * df['ask_size2'].values\n",
        "    df['bid_volume1'] = df['bid_price1'].values * df['bid_size1'].values\n",
        "    df['bid_volume2'] = df['bid_price2'].values * df['bid_size2'].values\n",
        "    df['ask_log_return1'] = df.groupby(['time_id'])[f'ask_volume1'].apply(log_return).fillna(0)\n",
        "    df['bid_log_return1'] = df.groupby(['time_id'])[f'bid_volume1'].apply(log_return).fillna(0)\n",
        "    df['diff_ask_log_return1'] = df['ask_log_return1'].diff().fillna(0)\n",
        "    df['diff_bid_log_return1'] = df['bid_log_return1'].diff().fillna(0)\n",
        "    df['ask_log_return2'] = df.groupby(['time_id'])[f'ask_volume2'].apply(log_return).fillna(0)\n",
        "    df['bid_log_return2'] = df.groupby(['time_id'])[f'bid_volume1'].apply(log_return).fillna(0)\n",
        "    df['diff_ask_log_return2'] = df['ask_log_return2'].diff().fillna(0)\n",
        "    df['diff_bid_log_return2'] = df['bid_log_return2'].diff().fillna(0)\n",
        "    \n",
        "    df['chg_count'] = (df[f\"log_return1\"]!=0).astype(np.int)\n",
        "    df['ask_price1_chg_count'] =  (df['ask_price1'].diff()!=0).astype(np.int)\n",
        "    df['ask_price2_chg_count'] =  (df['ask_price2'].diff()!=0).astype(np.int)\n",
        "    df['bid_price1_chg_count'] =  (df['bid_price1'].diff()!=0).astype(np.int)\n",
        "    df['bid_price2_chg_count'] =  (df['bid_price2'].diff()!=0).astype(np.int)\n",
        "    df['ask_size1_chg_count'] =  (df['ask_size1'].diff()!=0).astype(np.int)\n",
        "    df['ask_size2_chg_count'] =  (df['ask_size2'].diff()!=0).astype(np.int)\n",
        "    df['bid_size1_chg_count'] =  (df['bid_size1'].diff()!=0).astype(np.int)\n",
        "    df['bid_size2_chg_count'] =  (df['bid_size1'].diff()!=0).astype(np.int)\n",
        "    \n",
        "    df['tick_size'] = np.abs(df['ask_price1'].diff().fillna(0))\n",
        "    df.loc[df['tick_size']==0,'tick_size'] = 999\n",
        "    \n",
        "    df['pct_change_bid_size1'] = np.log1p(df['bid_size1'].pct_change())\n",
        "    df['pct_change_ask_size1'] = np.log1p(df['ask_size1'].pct_change())\n",
        "    \n",
        "    # Calculate wap balance\n",
        "    df['log_return_balance'] = np.log1p(df['log_return1'].values) / (df['log_return2'].values + 1)\n",
        "    # Calculate spread\n",
        "    df['price1_spread'] = (df['ask_price1'].values) / (df['bid_price1'].values + 1) - 1\n",
        "    df['price2_spread'] = (df['ask_price2'].values) / (df['bid_price2'].values + 1) - 1\n",
        "    \n",
        "    df['power1_spread'] = (np.log1p(df['ask_price1'].values * df['ask_size1'].values) / np.log1p(df['bid_price1'].values * df['bid_size1'].values)) - 1\n",
        "    df['power2_spread'] = (np.log1p(df['ask_price2'].values * df['ask_size2'].values) / np.log1p(df['bid_price2'].values * df['bid_size2'].values)) - 1\n",
        "    \n",
        "    df['bid_spread'] = (df['bid_price1'].values) / (df['bid_price2'].values + SMALL_F)\n",
        "    df['ask_spread'] = (df['ask_price1'].values) / (df['ask_price2'].values + SMALL_F)\n",
        "    df['bid_spread_log_return'] = df.groupby(['time_id'])['bid_spread'].apply(log_return).fillna(0)\n",
        "    df['ask_spread_log_return'] = df.groupby(['time_id'])['ask_spread'].apply(log_return).fillna(0)\n",
        "    df['bid_spread'] -= 1\n",
        "    df['ask_spread'] -= 1\n",
        "    \n",
        "    df['price1_spread_log_return'] = df.groupby(['time_id'])['price1_spread'].apply(log_return).fillna(0)\n",
        "    df['price2_spread_log_return'] = df.groupby(['time_id'])['price2_spread'].apply(log_return).fillna(0)\n",
        "\n",
        "    df['total_volume'] = (df['ask_size1'].values + df['ask_size2'].values) + (df['bid_size1'].values + df['bid_size2'].values)\n",
        "    df['volume_imbalance'] = ((df['ask_size1'].values + df['ask_size2'].values) / (df['total_volume'].values + SMALL_F)) - 1\n",
        "    \n",
        "    df['bid_ask_size1_mean'] = (df['bid_size1'].values + df['ask_size1'].values) / 2\n",
        "    \n",
        "    df_min_seconds = df.iloc[df.groupby(['time_id'])['log_return1'].idxmin()][['time_id','seconds_in_bucket']]\n",
        "    df_min_seconds.rename(columns={'seconds_in_bucket':'return_min_seconds_in_bucket'},inplace=True)\n",
        "    df_max_seconds = df.iloc[df.groupby(['time_id'])['log_return1'].idxmax()][['time_id','seconds_in_bucket']]\n",
        "    df_max_seconds.rename(columns={'seconds_in_bucket':'return_max_seconds_in_bucket'},inplace=True)\n",
        "    \n",
        "    if len(df) > 2:\n",
        "        df.drop(index=df.groupby(['time_id'])['seconds_in_bucket'].idxmin(),inplace=True)\n",
        "    \n",
        "    df_feature = df.groupby(by = ['time_id']).first()[[]].reset_index()\n",
        "    \n",
        "    # Dict for aggregations\n",
        "    create_feature_dict = {\n",
        "        'wap1':[spread_ratio],\n",
        "        'wap2':[spread_ratio],\n",
        "        'log_return1': [realized_volatility, np.std],\n",
        "        'log_return2': [realized_volatility, np.std],\n",
        "        'log_return1_half':[realized_volatility],\n",
        "        'log_return2_half':[realized_volatility],\n",
        "        'diff_log_return1':[np.sum],\n",
        "        'diff_log_return2':[np.sum],\n",
        "        'ask_log_return1': [realized_volatility],\n",
        "        'bid_log_return1': [realized_volatility],\n",
        "        'ask_log_return2': [realized_volatility],\n",
        "        'bid_log_return2': [realized_volatility],\n",
        "        'diff_ask_log_return1': [realized_volatility],\n",
        "        'diff_bid_log_return1': [realized_volatility],\n",
        "        'diff_ask_log_return2': [realized_volatility],\n",
        "        'diff_bid_log_return2': [realized_volatility],\n",
        "        'log_return1_minus': [np.sum, np.std ,smallest_mean],\n",
        "        'log_return1_plus': [np.sum, np.std ,largest_mean],\n",
        "        'log_return2_minus': [np.sum, np.std ,smallest_mean],\n",
        "        'log_return2_plus': [np.sum, np.std,largest_mean],\n",
        "        'chg_count':[np.sum],\n",
        "        'seconds_in_bucket':[count_unique],\n",
        "        'ask_price1_chg_count':[np.sum],\n",
        "        'ask_price2_chg_count':[np.sum],\n",
        "        'bid_price1_chg_count':[np.sum],\n",
        "        'bid_price2_chg_count':[np.sum],\n",
        "        'ask_size1_chg_count':[np.sum],\n",
        "        'ask_size2_chg_count':[np.sum],\n",
        "        'bid_size1_chg_count':[np.sum],\n",
        "        'bid_size2_chg_count':[np.sum],\n",
        "        'log_return_balance': [np.mean, largest_mean,spread_ratio],\n",
        "        'price1_spread': [np.mean, largest_mean, spread_ratio],\n",
        "        'price2_spread': [np.mean, largest_mean, spread_ratio],\n",
        "        'price1_spread_log_return':[np.std],\n",
        "        'price2_spread_log_return':[np.std],\n",
        "        'power1_spread': [np.mean, largest_mean, np.std, spread_ratio],\n",
        "        'power2_spread': [np.mean, largest_mean, np.std, spread_ratio],\n",
        "        'bid_spread': [np.mean, largest_mean, spread_ratio],\n",
        "        'bid_spread_log_return':[np.std],\n",
        "        'ask_spread': [np.mean, largest_mean, spread_ratio],\n",
        "        'ask_spread_log_return':[np.std],\n",
        "        'total_volume': [np.sum, spread_ratio],\n",
        "        'volume_imbalance': [np.mean, largest_mean, np.std,spread_ratio],\n",
        "        'pct_change_bid_size1': [realized_volatility, np.std],\n",
        "        'pct_change_ask_size1': [realized_volatility, np.std],\n",
        "        'tick_size':[np.min],\n",
        "        'bid_ask_size1_mean':[np.sum],\n",
        "    }\n",
        "    create_feature_time_dict= {\n",
        "        'log_return1': [realized_volatility, np.std],\n",
        "        'log_return2': [realized_volatility, np.std],\n",
        "        'chg_count':[np.sum],\n",
        "        'ask_price1_chg_count':[np.sum],\n",
        "        'ask_price2_chg_count':[np.sum],\n",
        "        'bid_price1_chg_count':[np.sum],\n",
        "        'bid_price2_chg_count':[np.sum],\n",
        "        'ask_size1_chg_count':[np.sum],\n",
        "        'ask_size2_chg_count':[np.sum],\n",
        "        'bid_size1_chg_count':[np.sum],\n",
        "        'bid_size2_chg_count':[np.sum],\n",
        "        'volume_imbalance':[np.mean],\n",
        "        'total_volume':[np.sum]\n",
        "    }\n",
        "    create_features_time_dict_2 = {\n",
        "        'price1_spread': [np.mean],\n",
        "        'price2_spread': [np.mean],\n",
        "        'bid_spread': [np.mean],\n",
        "        'ask_spread': [np.mean],\n",
        "    }\n",
        "    # Function to get group stats for different windows (seconds in bucket)\n",
        "    def get_stats(add_suffix = False):\n",
        "        # Group by the window\n",
        "        df_feature = df.groupby(['time_id']).agg(create_feature_dict)\n",
        "        # Rename columns joining suffix\n",
        "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
        "        # Add a suffix to differentiate windows\n",
        "        if add_suffix:\n",
        "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
        "        df_feature = df_feature.reset_index()\n",
        "        return df_feature\n",
        "\n",
        "    # Function to get group stats for different windows (seconds in bucket)\n",
        "    def get_stats_window(interval_sec, feature_dict , add_suffix = False):\n",
        "        # Group by the window\n",
        "        df_feature = df[(df['seconds_in_bucket'] >= interval_sec[0]) & (df['seconds_in_bucket'] <= interval_sec[1])].groupby(['time_id']).agg(feature_dict)\n",
        "        # Rename columns joining suffix\n",
        "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
        "        # Add a suffix to differentiate windows\n",
        "        if add_suffix:\n",
        "            df_feature = df_feature.add_suffix('_' + str(interval_sec[0]) + '_' + str(interval_sec[1]))\n",
        "        df_feature = df_feature.reset_index()\n",
        "        return df_feature\n",
        "    \n",
        "    # Get the stats for different windows\n",
        "    df_feature = get_stats(add_suffix = False)\n",
        "        \n",
        "    for interval_sec in [[150, 600],[300,600],[450,600]]:\n",
        "        tmp_df =   get_stats_window(interval_sec, create_feature_time_dict, add_suffix = True)\n",
        "        df_feature = df_feature.merge(tmp_df, how = 'left', on = 'time_id')\n",
        "\n",
        "    for interval_sec in [[0,300],[300,600]]:\n",
        "        tmp_df =   get_stats_window(interval_sec,create_features_time_dict_2, add_suffix = True)\n",
        "        df_feature = df_feature.merge(tmp_df, how = 'left', on = 'time_id')\n",
        "            \n",
        "    df_feature = df_feature.merge(df_min_seconds, how = 'left', on = 'time_id')\n",
        "    df_feature = df_feature.merge(df_max_seconds, how = 'left', on = 'time_id')\n",
        "        \n",
        "    # Drop unnecesary time_ids\n",
        "    df_feature.rename(columns={'time_id_':'time_id'}, inplace = True)\n",
        "    # Create row_id so we can merge\n",
        "    df_feature['stock_id'] = int(file_path.split('=')[1])\n",
        "    \n",
        "    for i in [1,2]:\n",
        "        df_feature[f'ask_bid_log_return{i}_realized_volatility'] = (df_feature[f'ask_log_return{i}_realized_volatility'].values) / (df_feature[f'bid_log_return{i}_realized_volatility'].values + SMALL_F) - 1\n",
        "        del df_feature[f'ask_log_return{i}_realized_volatility'],df_feature[f'bid_log_return{i}_realized_volatility']\n",
        "        df_feature[f'diff_ask_bid_log_return{i}_realized_volatility'] = (df_feature[f'diff_ask_log_return{i}_realized_volatility'].values) / (df_feature[f'diff_bid_log_return{i}_realized_volatility'].values + SMALL_F) - 1\n",
        "        del df_feature[f'diff_ask_log_return{i}_realized_volatility'],df_feature[f'diff_bid_log_return{i}_realized_volatility']\n",
        "        df_feature[f'bid_ask_price{i}_chg_per'] =  (df_feature[f'ask_price{i}_chg_count_sum']) / (df_feature[f'bid_price{i}_chg_count_sum'] + SMALL_F) - 1\n",
        "        df_feature[f'bid_ask_size{i}_chg_per'] =  (df_feature[f'ask_size{i}_chg_count_sum']) / (df_feature[f'bid_size{i}_chg_count_sum'] + SMALL_F) - 1\n",
        "        del df_feature[f'ask_price{i}_chg_count_sum'],df_feature[f'bid_price{i}_chg_count_sum']\n",
        "        del df_feature[f'ask_size{i}_chg_count_sum'],df_feature[f'bid_size{i}_chg_count_sum']\n",
        "\n",
        "        for interval_sec in [[150, 600],[300,600],[450,600]]:\n",
        "            df_feature[f'bid_ask_price{i}_chg_per_{interval_sec[0]}_{interval_sec[1]}'] = (df_feature[f'ask_price{i}_chg_count_sum_{interval_sec[0]}_{interval_sec[1]}']) / (df_feature[f'bid_price{i}_chg_count_sum_{interval_sec[0]}_{interval_sec[1]}'] + SMALL_F) - 1\n",
        "            df_feature[f'bid_ask_size{i}_chg_per_{interval_sec[0]}_{interval_sec[1]}'] = (df_feature[f'ask_size{i}_chg_count_sum_{interval_sec[0]}_{interval_sec[1]}']) / (df_feature[f'bid_size{i}_chg_count_sum_{interval_sec[0]}_{interval_sec[1]}'] + SMALL_F) - 1\n",
        "            del df_feature[f'ask_price{i}_chg_count_sum_{interval_sec[0]}_{interval_sec[1]}'],df_feature[f'bid_price{i}_chg_count_sum_{interval_sec[0]}_{interval_sec[1]}']\n",
        "            del df_feature[f'ask_size{i}_chg_count_sum_{interval_sec[0]}_{interval_sec[1]}'],df_feature[f'bid_size{i}_chg_count_sum_{interval_sec[0]}_{interval_sec[1]}']\n",
        "        \n",
        "    return df_feature"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3F4vyLM-dIa"
      },
      "source": [
        "def trade_preprocessor(file_path):\n",
        "    df = pd.read_parquet(file_path)\n",
        "    df = df.sort_values(['time_id','seconds_in_bucket'])\n",
        "    df['price'].fillna(1,inplace=True)\n",
        "    for c in ['size','order_count']:\n",
        "        df[c].fillna(0,inplace=True)\n",
        "        \n",
        "    df['log_return'] = df.groupby('time_id')['price'].apply(log_return).fillna(0)\n",
        "    df['log_return_half'] = df.groupby('time_id')['price'].apply(log_return_half).fillna(0)\n",
        "    df[\"amount\"] = df['price'].values * df['size'].values\n",
        "    df['price_size_chg_balance'] = ((df['price'].diff()!=0).astype(np.int)) / ((df['size'].diff()!=0).astype(np.int) + 1) - 1\n",
        "    \n",
        "    if len(df) > 2:\n",
        "        df.drop(index=df.groupby(['time_id'])['seconds_in_bucket'].idxmin(),inplace=True)\n",
        "    \n",
        "    df_feature = df.groupby(by = ['time_id']).first()[[]].reset_index()\n",
        "\n",
        "    # Dict for aggregations\n",
        "    create_feature_dict = {\n",
        "        'log_return':[realized_volatility],\n",
        "        'log_return_half':[realized_volatility],\n",
        "        'price':[spread_ratio],\n",
        "        'seconds_in_bucket':[count_unique],\n",
        "        'size': [np.sum],\n",
        "        'order_count': [np.sum],\n",
        "        'amount':[np.sum],\n",
        "        'price_size_chg_balance':[np.mean],\n",
        "    }\n",
        "    create_feature_time_dict= {\n",
        "        'log_return':[realized_volatility],\n",
        "        'seconds_in_bucket':[count_unique],\n",
        "        'amount':[np.sum],\n",
        "        'order_count': [np.sum],\n",
        "        'price_size_chg_balance':[np.mean],\n",
        "    }\n",
        "    \n",
        "    # Function to get group stats for different windows (seconds in bucket)\n",
        "    def get_stats(add_suffix = False):\n",
        "        # Group by the window\n",
        "        df_feature = df.groupby(['time_id']).agg(create_feature_dict)\n",
        "        # Rename columns joining suffix\n",
        "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
        "        # Add a suffix to differentiate windows\n",
        "        if add_suffix:\n",
        "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
        "        df_feature = df_feature.reset_index()\n",
        "        return df_feature\n",
        "    # Function to get group stats for different windows (seconds in bucket)\n",
        "    def get_stats_window(interval_sec, add_suffix = False):\n",
        "        # Group by the window\n",
        "        df_feature = df[(df['seconds_in_bucket'] >= interval_sec[0]) & (df['seconds_in_bucket'] <= interval_sec[1])].groupby(['time_id']).agg(create_feature_time_dict)\n",
        "        # Rename columns joining suffix\n",
        "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
        "        # Add a suffix to differentiate windows\n",
        "        if add_suffix:\n",
        "            df_feature = df_feature.add_suffix('_' + str(interval_sec[0]) + '_' + str(interval_sec[1]))\n",
        "        df_feature = df_feature.reset_index()\n",
        "        return df_feature\n",
        "    \n",
        "    # Get the stats for different windows\n",
        "    df_feature = get_stats(add_suffix = False)      \n",
        "        \n",
        "    for interval_sec in [[150, 600],[300,600],[450,600]]:\n",
        "        tmp_df =   get_stats_window(interval_sec, add_suffix = True)\n",
        "        df_feature = df_feature.merge(tmp_df, how = 'left', on = 'time_id')\n",
        "\n",
        "    df_feature = df_feature.add_prefix('trade_')\n",
        "    df_feature.rename(columns={'trade_time_id':'time_id'}, inplace = True)\n",
        "    df_feature['stock_id'] = int(file_path.split('=')[1])\n",
        "    \n",
        "    \n",
        "    return df_feature"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lT0fo0ys-i3S"
      },
      "source": [
        "def preprocessor(list_stock_ids, is_train=True):\n",
        "    dataType = 'test'\n",
        "    if is_train:\n",
        "        dataType = 'train'\n",
        "        \n",
        "    # Parrallel for loop\n",
        "    def for_joblib(stock_id):\n",
        "        file_path_book = data_dir + f\"book_{dataType}.parquet/stock_id={stock_id}\"\n",
        "        file_path_trade = data_dir + f\"trade_{dataType}.parquet/stock_id={stock_id}\"\n",
        "\n",
        "        # Preprocess book and trade data and merge them\n",
        "        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = ['stock_id','time_id'], how = 'left')\n",
        "        \n",
        "        # Return the merge dataframe\n",
        "        return df_tmp\n",
        "    \n",
        "    # Use parallel api to call paralle for loop\n",
        "    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n",
        "    # Concatenate all the dataframes that return from Parallel\n",
        "    df = pd.concat(df, ignore_index = True)\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTYKlatR-lij"
      },
      "source": [
        "def get_time_col(col,origin_include=True):\n",
        "    time_col = []\n",
        "    if origin_include:\n",
        "        time_col = [col]\n",
        "    for intervel_sec in [[150, 600],[300,600],[450,600]]:\n",
        "        time_col += [f'{col}_{intervel_sec[0]}_{intervel_sec[1]}']\n",
        "    return time_col\n",
        "\n",
        "def fillna_feature(df):\n",
        "    for c in ['trade_log_return_realized_volatility','log_return1_realized_volatility','log_return2_realized_volatility','trade_seconds_in_bucket_count_unique']:\n",
        "        df[c].fillna(0,inplace=True)\n",
        "        for intervel_sec in get_time_col(c,False):\n",
        "            df[intervel_sec].fillna(df[c],inplace=True)\n",
        "    return df\n",
        "\n",
        "def feat_scaling(df):\n",
        "    for c in ['log_return1_realized_volatility','log_return2_realized_volatility','trade_log_return_realized_volatility','trade_seconds_in_bucket_count_unique','trade_size_sum','trade_order_count_sum',\n",
        "              'chg_count_sum','trade_amount_sum','total_volume_sum','seconds_in_bucket_count_unique']:\n",
        "        df[c] = df[c].astype('float32')\n",
        "        for intervel_sec in get_time_col(c,False):\n",
        "            if intervel_sec in df.columns:\n",
        "                df[intervel_sec] = df[intervel_sec].astype('float32')\n",
        "                df[intervel_sec] /= df[c]\n",
        "    return df\n",
        "\n",
        "def book_seconds_scaling(df):\n",
        "    for c in ['chg_count_sum' ,'trade_seconds_in_bucket_count_unique']:\n",
        "        df[c] = df[c].astype('float32')\n",
        "        df[c] /= df['seconds_in_bucket_count_unique']\n",
        "    return df\n",
        "def time_fe(df):\n",
        "    cols = get_time_col('log_return1_realized_volatility') + get_time_col('log_return2_realized_volatility') + ['trade_log_return_realized_volatility']\n",
        "    for agg_col in [\"time_id\"]:\n",
        "        for agg_func in [\"mean\", largest_mean, \"std\", smallest_mean]:\n",
        "            agg_df = df.groupby(agg_col)[cols].agg(agg_func)\n",
        "            agg_df.columns = [f\"{agg_col}_{agg_func}_{col}\" for col in agg_df.columns]\n",
        "            df = df.merge(agg_df.reset_index(), on=agg_col, how=\"left\")\n",
        "    \n",
        "    cols = get_time_col('chg_count_sum') + get_time_col('trade_seconds_in_bucket_count_unique')\n",
        "    \n",
        "    for agg_col in [\"time_id\"]:\n",
        "        for agg_func in [\"mean\", \"std\"]:\n",
        "            agg_df = df.groupby(agg_col)[cols].agg(agg_func)\n",
        "            agg_df.columns = [f\"{agg_col}_{agg_func}_{col}\" for col in agg_df.columns]\n",
        "            df = df.merge(agg_df.reset_index(), on=agg_col, how=\"left\")\n",
        "    \n",
        "    cols = ['log_return1_minus_smallest_mean','power1_spread_mean','power2_spread_mean','bid_ask_price1_chg_per','bid_ask_price2_chg_per','bid_ask_size1_chg_per','bid_ask_size2_chg_per'] + get_time_col('trade_amount_sum') + get_time_col('trade_order_count_sum') + get_time_col('total_volume_sum') + get_time_col('volume_imbalance_mean')\n",
        "    \n",
        "    for agg_col in [\"time_id\"]:\n",
        "        for agg_func in [\"mean\"]:\n",
        "            agg_df = df.groupby(agg_col)[cols].agg(agg_func)\n",
        "            agg_df.columns = [f\"{agg_col}_{agg_func}_{col}\" for col in agg_df.columns]\n",
        "            df = df.merge(agg_df.reset_index(), on=agg_col, how=\"left\")\n",
        "        \n",
        "    cols = ['chg_count_sum','trade_order_count_sum']\n",
        "    for agg_col in [\"time_id\"]:\n",
        "        for agg_func in [\"sum\"]:\n",
        "            agg_df = df.groupby(agg_col)[cols].agg(agg_func)\n",
        "            agg_df.columns = [f\"{agg_col}_{agg_func}_{col}\" for col in agg_df.columns]\n",
        "            df = df.merge(agg_df.reset_index(), on=agg_col, how=\"left\")\n",
        "    \n",
        "    cols = ['stock_id']\n",
        "    for agg_col in [\"time_id\"]:\n",
        "        for agg_func in [count_unique]:\n",
        "            agg_df = df.groupby(agg_col)[cols].agg(agg_func)\n",
        "            agg_df.columns = [f\"{agg_col}_{agg_func}_{col}\" for col in agg_df.columns]\n",
        "            df = df.merge(agg_df.reset_index(), on=agg_col, how=\"left\")    \n",
        "        \n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZqrZYHx-uVa"
      },
      "source": [
        "# Read train and test\n",
        "train, test = read_train_test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHMI-ZQ5-xYi"
      },
      "source": [
        "# Get unique stock ids \n",
        "test_stock_ids = test['stock_id'].unique()\n",
        "# Preprocess them using Parallel and our single stock id functions\n",
        "test_ = preprocessor(test_stock_ids, False)\n",
        "test = test.merge(test_, on = ['time_id','stock_id'], how = 'left')\n",
        "del test_\n",
        "test = fillna_feature(test)\n",
        "test = feat_scaling(test)\n",
        "test = book_seconds_scaling(test)\n",
        "test = time_fe(test)\n",
        "\n",
        "test['time_id_sum_chg_count_sum'] = (test['chg_count_sum']) / (test['time_id_sum_chg_count_sum'] + 1) - 1\n",
        "test['time_id_sum_trade_order_count_sum'] = (test['trade_order_count_sum']) / (test['time_id_sum_trade_order_count_sum'] + 1) - 1\n",
        "test['real_volume_per'] = (test['trade_size_sum']) / (test['bid_ask_size1_mean_sum'] + 1) - 1\n",
        "test['log_return_half_spread'] = ((test['log_return1_half_realized_volatility']) / (test['log_return2_half_realized_volatility'] + SMALL_F)) - 1\n",
        "\n",
        "for c in ['time_id_mean_total_volume_sum','time_id_mean_trade_amount_sum','time_id_mean_trade_order_count_sum','total_volume_sum','trade_size_sum','trade_order_count_sum','bid_ask_size1_mean_sum']:\n",
        "    del test[c]\n",
        "\n",
        "for c in ['bid_ask_price1_chg_per','bid_ask_price2_chg_per']:\n",
        "    for intervel_sec in get_time_col(c,False):\n",
        "        test.loc[test[intervel_sec]>10,intervel_sec] = test[c]\n",
        "    \n",
        "# Get unique stock ids \n",
        "train_stock_ids = train['stock_id'].unique()\n",
        "# Preprocess them using Parallel and our single stock id functions\n",
        "train_ = preprocessor(train_stock_ids, True)\n",
        "train = train.merge(train_, on = ['time_id','stock_id'], how = 'left')\n",
        "del train_\n",
        "train = fillna_feature(train)\n",
        "train = feat_scaling(train)\n",
        "train = book_seconds_scaling(train)\n",
        "train = time_fe(train)\n",
        "\n",
        "train['time_id_sum_chg_count_sum'] = (train['chg_count_sum']) / (train['time_id_sum_chg_count_sum'] + 1) - 1\n",
        "train['time_id_sum_trade_order_count_sum'] = (train['trade_order_count_sum']) / (train['time_id_sum_trade_order_count_sum'] + 1) - 1\n",
        "train['real_volume_per'] = (train['trade_size_sum']) / (train['bid_ask_size1_mean_sum'] + 1) - 1\n",
        "train['log_return_half_spread'] = ((train['log_return1_half_realized_volatility']) / (train['log_return2_half_realized_volatility'] + SMALL_F)) - 1\n",
        "\n",
        "for c in ['time_id_mean_total_volume_sum','time_id_mean_trade_amount_sum','time_id_mean_trade_order_count_sum','total_volume_sum','trade_size_sum','trade_order_count_sum','bid_ask_size1_mean_sum']:\n",
        "    del train[c]\n",
        "    \n",
        "for c in ['bid_ask_price1_chg_per','bid_ask_price2_chg_per']:\n",
        "    for intervel_sec in get_time_col(c,False):\n",
        "        train.loc[train[intervel_sec]>10,intervel_sec] = train[c]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKLmYB8s-zMy"
      },
      "source": [
        "for c in train.columns:\n",
        "    l = train[c].isnull().sum()\n",
        "    if l > 0:\n",
        "        print(c, l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kU3bBuaJ-13S"
      },
      "source": [
        "def train_and_evaluate(train, test, ex_col, submission_file, model_num):\n",
        "    model_name = 'lgb' + str(model_num)\n",
        "    pred_name = 'pred_{}'.format(model_name)\n",
        "    \n",
        "    for c in train.columns:\n",
        "        if 'pred' in c:\n",
        "            del train[c]\n",
        "    \n",
        "    train[pred_name] = 0\n",
        "    feat =  [col for col in train.columns if col not in ['time_id','target', pred_name]]\n",
        "\n",
        "    params = {\n",
        "        'objective': 'rmse',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'max_depth': 6,\n",
        "        'max_bin':len(train['stock_id'].unique()),\n",
        "        'min_data_in_leaf':300,\n",
        "        'learning_rate': 0.05,\n",
        "        'subsample': 0.8,\n",
        "        'subsample_freq': 4,\n",
        "        'feature_fraction': 0.4,\n",
        "        'lambda_l1': 0.4,\n",
        "        'lambda_l2': 0.4,\n",
        "        'seed':model_num,\n",
        "        'feature_fraction_seed': model_num,\n",
        "        'bagging_seed': model_num,\n",
        "        'drop_seed': model_num,\n",
        "        'data_random_seed': model_num,\n",
        "        \"tree_learner\": 'voting',\n",
        "        'verbose': -1,\n",
        "        'metric': 'None',}\n",
        "    \n",
        "    # Split features and target\n",
        "    x = train.drop(['target', 'time_id'] + ex_col , axis = 1)\n",
        "    y = train['target'].values\n",
        "    x_test = test.drop(['row_id', 'time_id'] + ex_col, axis = 1)\n",
        "    \n",
        "    # Create out of folds array\n",
        "    oof_predictions = np.zeros(x.shape[0])\n",
        "    # Create test array to store predictions\n",
        "    test_predictions = np.zeros(x_test.shape[0])\n",
        "    # Create a KFold object\n",
        "    n_folds = 7\n",
        "    kfold = KFold(n_splits = n_folds, random_state = model_num, shuffle = True)\n",
        "    publisher_train = train['time_id']\n",
        "    unique_publisher = train['time_id'].unique()\n",
        "    # Iterate through each fold\n",
        "    for fold, (tr_group_idx, va_group_idx) in enumerate(kfold.split(unique_publisher)):\n",
        "        tr_groups, va_groups = unique_publisher[tr_group_idx], unique_publisher[va_group_idx]\n",
        "        is_tr = publisher_train.isin(tr_groups)\n",
        "        is_va = publisher_train.isin(va_groups)\n",
        "        \n",
        "        x_train, x_val = x[is_tr], x[is_va]\n",
        "        y_train, y_val = y[is_tr], y[is_va]\n",
        "        \n",
        "        print(f'Training fold {fold + 1}, train={len(x_train)} valid={len(x_val)}')\n",
        "        # Root mean squared percentage error weights\n",
        "        train_weights = 1 / np.square(y_train)\n",
        "        val_weights = 1 / np.square(y_val)\n",
        "        train_dataset = lgb.Dataset(x_train[feat], y_train, weight = train_weights, categorical_feature = ['stock_id'])\n",
        "        val_dataset = lgb.Dataset(x_val[feat], y_val, weight = val_weights, categorical_feature = ['stock_id'])\n",
        "        model = lgb.train(params = params, \n",
        "                          train_set = train_dataset, \n",
        "                          valid_sets = [train_dataset, val_dataset], \n",
        "                          num_boost_round = 10000, \n",
        "                          early_stopping_rounds = 80, \n",
        "                          verbose_eval = 200,\n",
        "                          feval = feval_rmspe)\n",
        "        # Add predictions to the out of folds array\n",
        "        oof_predictions[is_va] = model.predict(x_val[feat])\n",
        "        oof_predictions[oof_predictions <= SMALL_F] = SMALL_F \n",
        "        train.loc[is_va, pred_name] += oof_predictions[is_va]\n",
        "        # Predict the test set\n",
        "        if len(x_test) > 0:\n",
        "            test_pred = model.predict(x_test[feat]) / TARGET_SCALE\n",
        "            test_pred[test_pred < SMALL_F] = SMALL_F\n",
        "            test_predictions += np.sqrt(test_pred) / n_folds\n",
        "    train.to_csv(model_name,index = False) \n",
        "    rmspe_score = rmspe(np.sqrt(y)/TARGET_SCALE, np.sqrt(oof_predictions)/TARGET_SCALE)\n",
        "    print(f'Our out of folds RMSPE is {rmspe_score}')\n",
        "    test['target'] = test_predictions\n",
        "    test[['row_id', 'target']].to_csv(submission_file,index = False)\n",
        "    lgb.plot_importance(model,max_num_features=50,figsize=(10,10))\n",
        "    # Return test predictions\n",
        "    return test_predictions, oof_predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07qcEJyJ_QUE"
      },
      "source": [
        "print(len(train.columns))\n",
        "for c in train.columns:\n",
        "    print(c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Op4k1YIS_WK8"
      },
      "source": [
        "#地合いで分割(傾向によって各特徴の重みも変わるし、target変数も正規分布に近くなると)\n",
        "time_quantile_dict = {}\n",
        "train_time_group_dict = {}\n",
        "col = 'log_return1_realized_volatility'\n",
        "df_feature = train.groupby(by = ['time_id']).sum()[[col]].reset_index()\n",
        "for p in [0.2,0.5,0.8]:\n",
        "    df_quantile = df_feature[['time_id',col]].quantile([p]).reset_index()\n",
        "    df_quantile = df_quantile[['time_id',col]]\n",
        "    time_quantile_dict[p] = df_quantile[col][0]\n",
        "\n",
        "\n",
        "train_time_group_dict[0] = df_feature.loc[df_feature[col] <= time_quantile_dict[0.8]]['time_id'].unique()\n",
        "print('0.8 under=',len(train_time_group_dict[0]))\n",
        "train_time_group_dict[1] = df_feature.loc[df_feature[col] >= time_quantile_dict[0.2]]['time_id'].unique()      \n",
        "print('0.2 over=',len(train_time_group_dict[1]))\n",
        "print(len(set(np.concatenate([train_time_group_dict[0],train_time_group_dict[1]]))))\n",
        "\n",
        "test_time_group_dict = {}\n",
        "df_feature = test.groupby(by = ['time_id']).sum()[[col]].reset_index()\n",
        "test_time_group_dict[0] = df_feature.loc[df_feature[col] <= time_quantile_dict[0.8]]['time_id'].unique()\n",
        "print('0.8 under=',len(test_time_group_dict[0]))\n",
        "test_time_group_dict[1] = df_feature.loc[df_feature[col] >= time_quantile_dict[0.2]]['time_id'].unique()      \n",
        "print('0.2 over=',len(test_time_group_dict[1]))\n",
        "print(len(set(np.concatenate([test_time_group_dict[0],test_time_group_dict[1]]))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dty0sgxr_YCj"
      },
      "source": [
        "for g in range(0, len(train_time_group_dict)):\n",
        "    test_predictions, valid_predictions = train_and_evaluate(train.loc[train['time_id'].isin(train_time_group_dict[g])].reset_index(drop=True), test[test['time_id'].isin(test_time_group_dict[g])].reset_index(drop=True),[], f\"group{g}.csv\",g)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "069aAHgd_a8K"
      },
      "source": [
        "result = {}\n",
        "result[0] = pd.read_csv(f'lgb{0}')[['stock_id','time_id',f'pred_lgb{0}','target']]\n",
        "result[0].rename(columns={f'pred_lgb{0}':'pred'},inplace=True)\n",
        "for g in range(1, len(train_time_group_dict)):\n",
        "    result[g] = pd.read_csv(f'lgb{g}')[['stock_id','time_id',f'pred_lgb{g}','target']]\n",
        "    result[0] = result[0].merge(result[g][['stock_id','time_id',f'pred_lgb{g}']],on=['stock_id','time_id'],how='left')\n",
        "    \n",
        "result[0]['pred'].fillna(result[0]['pred_lgb1'],inplace=True)\n",
        "result[0]['pred_lgb1'].fillna(result[0]['pred'],inplace=True)\n",
        "result[0]['pred'] = (result[0]['pred'] + result[0]['pred_lgb1']) / 2\n",
        "print(rmspe(np.sqrt(result[0]['target'].values),np.sqrt(result[0]['pred'].values)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gvplut4h_cui"
      },
      "source": [
        "result[0][['target','pred','stock_id']].groupby('stock_id').mean().plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pvHFXc7_etD"
      },
      "source": [
        "preds= {}\n",
        "preds[0] = pd.read_csv(f'group{0}.csv',index_col='row_id')\n",
        "for g in range(1, len(test_time_group_dict)):\n",
        "    preds[g] = pd.read_csv(f'group{g}.csv',index_col='row_id')\n",
        "    preds[g].rename(columns={'target':f'target_{g}'},inplace=True)\n",
        "    preds[0] = preds[0].merge(preds[g][[f'target_{g}']],left_index=True,right_index=True,how='outer')\n",
        "preds[0]['target'].fillna(preds[0]['target_1'],inplace=True)\n",
        "preds[0]['target_1'].fillna(preds[0]['target'],inplace=True)\n",
        "preds[0]['target'] = (preds[0]['target'] + preds[0]['target_1']) / 2\n",
        "    \n",
        "sub = preds[0].reset_index()\n",
        "test[['row_id']].merge(sub[['row_id', 'target']],on='row_id',how='left').to_csv('submission.csv',index = False)\n",
        "pd.read_csv('submission.csv',index_col='row_id')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBuq6P8g_ggb"
      },
      "source": [
        "for g in range(0, len(train_time_group_dict)):\n",
        "    test_predictions, valid_predictions = train_and_evaluate(train.loc[train['time_id'].isin(train_time_group_dict[g])].reset_index(drop=True), test[test['time_id'].isin(test_time_group_dict[g])].reset_index(drop=True),[], f\"group{g}.csv\",g+10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0YB010l_jPC"
      },
      "source": [
        "result_2 = {}\n",
        "result_2[0] = pd.read_csv(f'lgb{0}')[['stock_id','time_id',f'pred_lgb{0}','target']]\n",
        "result_2[0].rename(columns={f'pred_lgb{0}':'pred'},inplace=True)\n",
        "for g in range(1, len(train_time_group_dict)):\n",
        "    result_2[g] = pd.read_csv(f'lgb{g+10}')[['stock_id','time_id',f'pred_lgb{g+10}','target']]\n",
        "    result_2[0] = result_2[0].merge(result_2[g][['stock_id','time_id',f'pred_lgb{g+10}']],on=['stock_id','time_id'],how='left')\n",
        "    \n",
        "result_2[0]['pred'].fillna(result_2[0]['pred_lgb11'],inplace=True)\n",
        "result_2[0]['pred_lgb11'].fillna(result_2[0]['pred'],inplace=True)\n",
        "result_2[0]['pred'] = (result_2[0]['pred'] + result_2[0]['pred_lgb11']) / 2\n",
        "print(rmspe(np.sqrt(result_2[0]['target'].values),np.sqrt(result_2[0]['pred'].values)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPq9ypYY_kvd"
      },
      "source": [
        "result_2[0][['target','pred','stock_id']].groupby('stock_id').mean().plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LuB--gb_mfN"
      },
      "source": [
        "preds= {}\n",
        "preds[0] = pd.read_csv(f'group{0}.csv',index_col='row_id')\n",
        "for g in range(1, len(test_time_group_dict)):\n",
        "    preds[g] = pd.read_csv(f'group{g}.csv',index_col='row_id')\n",
        "    preds[g].rename(columns={'target':f'target_{g}'},inplace=True)\n",
        "    preds[0] = preds[0].merge(preds[g][[f'target_{g}']],left_index=True,right_index=True,how='outer')\n",
        "preds[0]['target'].fillna(preds[0]['target_1'],inplace=True)\n",
        "preds[0]['target_1'].fillna(preds[0]['target'],inplace=True)\n",
        "preds[0]['target'] = (preds[0]['target'] + preds[0]['target_1']) / 2\n",
        "    \n",
        "sub = preds[0].reset_index()\n",
        "test[['row_id']].merge(sub[['row_id', 'target']],on='row_id',how='left').to_csv('submission_2.csv',index = False)\n",
        "pd.read_csv('submission_2.csv',index_col='row_id')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kn-onAZy_oZs"
      },
      "source": [
        "sub = pd.read_csv('submission.csv',index_col='row_id')\n",
        "sub2 = pd.read_csv('submission_2.csv',index_col='row_id')\n",
        "sub['target'] = (sub['target'] + sub2['target']) / 2\n",
        "sub.to_csv('submission.csv')\n",
        "pd.read_csv('submission.csv',index_col='row_id')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
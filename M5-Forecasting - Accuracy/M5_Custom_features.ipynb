{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M5 - Custom features.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyP+SQLMP0QxOtF3RGQ9d1oQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pea-sys/Kaggle_Public/blob/master/M5_Custom_features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI45Ed7fPBcV",
        "colab_type": "text"
      },
      "source": [
        "feature engineering notebook\n",
        "以下notebookと同じ  \n",
        "[Kaggle Notebook](https://www.kaggle.com/kyakovlev/m5-custom-features)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60Ugx85gotSn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "7c084229-2585-469f-a4ac-a26c5919c107"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9GfHRF7pF3m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7e11f433-5e63-4d72-8f49-fb73fdb8e1d5"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/m5-forecasting-accuracy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/m5-forecasting-accuracy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5QUUyFnpM7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## In this kernel I would like to show: \n",
        "## 1. FE creation approaches\n",
        "## 2. Sequential fe validation\n",
        "## 3. Dimension reduction\n",
        "## 4. FE validation by Permutation importance\n",
        "## 5. Mean encodings\n",
        "## 6. Parallelization for FE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6G7UNRt4pRsp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import os, sys, gc, warnings, psutil, random\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nl_K4_ytpTwJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        },
        "outputId": "cbe00d06-6f9c-41fb-ffab-31438c1f8d8d"
      },
      "source": [
        "########################### Load data\n",
        "########################### Basic features were created here:\n",
        "########################### https://www.kaggle.com/kyakovlev/m5-simple-fe\n",
        "#################################################################################\n",
        "\n",
        "# Read data\n",
        "grid_df = pd.concat([pd.read_pickle('grid_part_1.pkl'),\n",
        "                     pd.read_pickle('grid_part_2.pkl').iloc[:,2:],\n",
        "                     pd.read_pickle('grid_part_3.pkl').iloc[:,2:]],\n",
        "                     axis=1)\n",
        "\n",
        "# Subsampling\n",
        "# to make all calculations faster.\n",
        "# Keep only 5% of original ids.\n",
        "keep_id = np.array_split(list(grid_df['id'].unique()), 20)[0]\n",
        "grid_df = grid_df[grid_df['id'].isin(keep_id)].reset_index(drop=True)\n",
        "\n",
        "# Let's \"inspect\" our grid DataFrame\n",
        "grid_df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3002725 entries, 0 to 3002724\n",
            "Data columns (total 34 columns):\n",
            " #   Column            Dtype   \n",
            "---  ------            -----   \n",
            " 0   id                category\n",
            " 1   item_id           category\n",
            " 2   dept_id           category\n",
            " 3   cat_id            category\n",
            " 4   store_id          category\n",
            " 5   state_id          category\n",
            " 6   d                 int16   \n",
            " 7   sales             float64 \n",
            " 8   release           int16   \n",
            " 9   sell_price        float16 \n",
            " 10  price_max         float16 \n",
            " 11  price_min         float16 \n",
            " 12  price_std         float16 \n",
            " 13  price_mean        float16 \n",
            " 14  price_norm        float16 \n",
            " 15  price_nunique     float16 \n",
            " 16  item_nunique      int16   \n",
            " 17  price_momentum    float16 \n",
            " 18  price_momentum_m  float16 \n",
            " 19  price_momentum_y  float16 \n",
            " 20  event_name_1      category\n",
            " 21  event_type_1      category\n",
            " 22  event_name_2      category\n",
            " 23  event_type_2      category\n",
            " 24  snap_CA           category\n",
            " 25  snap_TX           category\n",
            " 26  snap_WI           category\n",
            " 27  tm_d              int8    \n",
            " 28  tm_w              int8    \n",
            " 29  tm_m              int8    \n",
            " 30  tm_y              int8    \n",
            " 31  tm_wm             int8    \n",
            " 32  tm_dw             int8    \n",
            " 33  tm_w_end          int8    \n",
            "dtypes: category(13), float16(10), float64(1), int16(3), int8(7)\n",
            "memory usage: 162.0 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpsL9-YdpfFY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "9046ed42-5bf6-44ad-f301-60c8c5269f05"
      },
      "source": [
        "########################### Baseline model\n",
        "#################################################################################\n",
        "\n",
        "# We will need some global VARS for future\n",
        "\n",
        "SEED = 42             # Our random seed for everything\n",
        "random.seed(SEED)     # to make all tests \"deterministic\"\n",
        "np.random.seed(SEED)\n",
        "N_CORES = psutil.cpu_count()     # Available CPU cores\n",
        "\n",
        "TARGET = 'sales'      # Our Target\n",
        "END_TRAIN = 1941      # And we will use last 28 days as validation\n",
        "\n",
        "# Drop some items from \"TEST\" set part (1914...)\n",
        "grid_df = grid_df[grid_df['d']<=END_TRAIN].reset_index(drop=True)\n",
        "\n",
        "# Features that we want to exclude from training\n",
        "remove_features = ['id','d',TARGET]\n",
        "\n",
        "# Our baseline model serves\n",
        "# to do fast checks of\n",
        "# new features performance \n",
        "\n",
        "# We will use LightGBM for our tests\n",
        "import lightgbm as lgb\n",
        "lgb_params = {\n",
        "                    'boosting_type': 'gbdt',         # Standart boosting type\n",
        "                    'objective': 'regression',       # Standart loss for RMSE\n",
        "                    'metric': ['rmse'],              # as we will use rmse as metric \"proxy\"\n",
        "                    'subsample': 0.8,                \n",
        "                    'subsample_freq': 1,\n",
        "                    'learning_rate': 0.05,           # 0.5 is \"fast enough\" for us\n",
        "                    'num_leaves': 2**7-1,            # We will need model only for fast check\n",
        "                    'min_data_in_leaf': 2**8-1,      # So we want it to train faster even with drop in generalization \n",
        "                    'feature_fraction': 0.8,\n",
        "                    'n_estimators': 5000,            # We don't want to limit training (you can change 5000 to any big enough number)\n",
        "                    'early_stopping_rounds': 30,     # We will stop training almost immediately (if it stops improving) \n",
        "                    'seed': SEED,\n",
        "                    'verbose': -1,\n",
        "                } \n",
        "\n",
        "## RMSE\n",
        "def rmse(y, y_pred):\n",
        "    return np.sqrt(np.mean(np.square(y - y_pred)))\n",
        "\n",
        "# Small function to make fast features tests\n",
        "# estimator = make_fast_test(grid_df)\n",
        "# it will return lgb booster for future analisys\n",
        "def make_fast_test(df):\n",
        "\n",
        "    features_columns = [col for col in list(df) if col not in remove_features]\n",
        "\n",
        "    tr_x, tr_y = df[df['d']<=(END_TRAIN-28)][features_columns], df[df['d']<=(END_TRAIN-28)][TARGET]              \n",
        "    vl_x, v_y = df[df['d']>(END_TRAIN-28)][features_columns], df[df['d']>(END_TRAIN-28)][TARGET]\n",
        "    \n",
        "    train_data = lgb.Dataset(tr_x, label=tr_y)\n",
        "    valid_data = lgb.Dataset(vl_x, label=v_y)\n",
        "    \n",
        "    estimator = lgb.train(\n",
        "                            lgb_params,\n",
        "                            train_data,\n",
        "                            valid_sets = [train_data,valid_data],\n",
        "                            verbose_eval = 500,\n",
        "                        )\n",
        "    \n",
        "    return estimator\n",
        "\n",
        "# Make baseline model\n",
        "baseline_model = make_fast_test(grid_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 30 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[379]\ttraining's rmse: 2.79812\tvalid_1's rmse: 2.39787\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KROqbCAUpiEI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "f2f30cbf-928d-4416-f385-7065f78b336e"
      },
      "source": [
        "########################### Lets test our normal Lags (7 days)\n",
        "########################### Some more info about lags here:\n",
        "########################### https://www.kaggle.com/kyakovlev/m5-lags-features\n",
        "#################################################################################\n",
        "\n",
        "# Small helper to make lags creation faster\n",
        "from multiprocessing import Pool                # Multiprocess Runs\n",
        "\n",
        "## Multiprocessing Run.\n",
        "# :t_split - int of lags days                   # type: int\n",
        "# :func - Function to apply on each split       # type: python function\n",
        "# This function is NOT 'bulletproof', be carefull and pass only correct types of variables.\n",
        "## Multiprocess Runs\n",
        "def df_parallelize_run(func, t_split):\n",
        "    num_cores = np.min([N_CORES,len(t_split)])\n",
        "    pool = Pool(num_cores)\n",
        "    df = pd.concat(pool.map(func, t_split), axis=1)\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "    return df\n",
        "\n",
        "def make_normal_lag(lag_day):\n",
        "    lag_df = grid_df[['id','d',TARGET]] # not good to use df from \"global space\"\n",
        "    col_name = 'sales_lag_'+str(lag_day)\n",
        "    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(lag_day)).astype(np.float16)\n",
        "    return lag_df[[col_name]]\n",
        "\n",
        "# Launch parallel lag creation\n",
        "# and \"append\" to our grid\n",
        "LAGS_SPLIT = [col for col in range(1,1+7)]\n",
        "grid_df = pd.concat([grid_df, df_parallelize_run(make_normal_lag,LAGS_SPLIT)], axis=1)\n",
        "\n",
        "# Make features test\n",
        "test_model = make_fast_test(grid_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 30 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[325]\ttraining's rmse: 2.56457\tvalid_1's rmse: 2.26084\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxD1ZXXVpl4k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "outputId": "819896f1-1142-45fc-9240-4b0d20e8f6c4"
      },
      "source": [
        "########################### Permutation importance Test\n",
        "########################### https://www.kaggle.com/dansbecker/permutation-importance @dansbecker\n",
        "#################################################################################\n",
        "\n",
        "# Let's creat validation dataset and features\n",
        "features_columns = [col for col in list(grid_df) if col not in remove_features]\n",
        "validation_df = grid_df[grid_df['d']>(END_TRAIN-28)].reset_index(drop=True)\n",
        "\n",
        "# Make normal prediction with our model and save score\n",
        "validation_df['preds'] = test_model.predict(validation_df[features_columns])\n",
        "base_score = rmse(validation_df[TARGET], validation_df['preds'])\n",
        "print('Standart RMSE', base_score)\n",
        "\n",
        "\n",
        "# Now we are looping over all our numerical features\n",
        "for col in features_columns:\n",
        "    \n",
        "    # We will make validation set copy to restore\n",
        "    # features states on each run\n",
        "    temp_df = validation_df.copy()\n",
        "    \n",
        "    # Error here appears if we have \"categorical\" features and can't \n",
        "    # do np.random.permutation without disrupt categories\n",
        "    # so we need to check if feature is numerical\n",
        "    if temp_df[col].dtypes.name != 'category':\n",
        "        temp_df[col] = np.random.permutation(temp_df[col].values)\n",
        "        temp_df['preds'] = test_model.predict(temp_df[features_columns])\n",
        "        cur_score = rmse(temp_df[TARGET], temp_df['preds'])\n",
        "        \n",
        "        # If our current rmse score is less than base score\n",
        "        # it means that feature most probably is a bad one\n",
        "        # and our model is learning on noise\n",
        "        print(col, np.round(cur_score - base_score, 4))\n",
        "\n",
        "# Remove Temp data\n",
        "del temp_df, validation_df\n",
        "\n",
        "# Remove test features\n",
        "# As we will compare performance with baseline model for now\n",
        "keep_cols = [col for col in list(grid_df) if 'sales_lag_' not in col]\n",
        "grid_df = grid_df[keep_cols]\n",
        "\n",
        "\n",
        "# Results:\n",
        "## Lags with 1 days shift (nearest past) are important\n",
        "## Some other features are not important and probably just noise\n",
        "## Better make several Permutation runs to confirm useless of the feature\n",
        "## link again https://www.kaggle.com/dansbecker/permutation-importance @dansbecker\n",
        "\n",
        "## price_nunique -0.002 : strong negative values are most probably noise\n",
        "## price_max -0.0002 : values close to 0 need deeper investigation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Standart RMSE 2.260843574932562\n",
            "release 0.0\n",
            "sell_price 0.0038\n",
            "price_max 0.0001\n",
            "price_min 0.0009\n",
            "price_std 0.0029\n",
            "price_mean 0.0035\n",
            "price_norm 0.0075\n",
            "price_nunique -0.0001\n",
            "item_nunique -0.0\n",
            "price_momentum -0.0001\n",
            "price_momentum_m 0.0034\n",
            "price_momentum_y 0.0005\n",
            "tm_d 0.0106\n",
            "tm_w 0.0003\n",
            "tm_m -0.0001\n",
            "tm_y 0.0\n",
            "tm_wm 0.0002\n",
            "tm_dw 0.1403\n",
            "tm_w_end 0.0087\n",
            "sales_lag_1 0.5897\n",
            "sales_lag_2 0.0465\n",
            "sales_lag_3 0.0219\n",
            "sales_lag_4 0.012\n",
            "sales_lag_5 0.0183\n",
            "sales_lag_6 0.019\n",
            "sales_lag_7 0.0448\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDjMxcgvppA_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "outputId": "98cc7605-67bb-421e-df1f-94a03f3f1ec1"
      },
      "source": [
        "########################### Lets test far away Lags (7 days with 56 days shift)\n",
        "########################### and check permutation importance\n",
        "#################################################################################\n",
        "\n",
        "LAGS_SPLIT = [col for col in range(56,56+7)]\n",
        "grid_df = pd.concat([grid_df, df_parallelize_run(make_normal_lag,LAGS_SPLIT)], axis=1)\n",
        "test_model = make_fast_test(grid_df)\n",
        "\n",
        "features_columns = [col for col in list(grid_df) if col not in remove_features]\n",
        "validation_df = grid_df[grid_df['d']>(END_TRAIN-28)].reset_index(drop=True)\n",
        "validation_df['preds'] = test_model.predict(validation_df[features_columns])\n",
        "base_score = rmse(validation_df[TARGET], validation_df['preds'])\n",
        "print('Standart RMSE', base_score)\n",
        "\n",
        "for col in features_columns:\n",
        "    temp_df = validation_df.copy()\n",
        "    if temp_df[col].dtypes.name != 'category':\n",
        "        temp_df[col] = np.random.permutation(temp_df[col].values)\n",
        "        temp_df['preds'] = test_model.predict(temp_df[features_columns])\n",
        "        cur_score = rmse(temp_df[TARGET], temp_df['preds'])\n",
        "        print(col, np.round(cur_score - base_score, 4))\n",
        "\n",
        "del temp_df, validation_df\n",
        "        \n",
        "# Remove test features\n",
        "# As we will compare performance with baseline model for now\n",
        "keep_cols = [col for col in list(grid_df) if 'sales_lag_' not in col]\n",
        "grid_df = grid_df[keep_cols]\n",
        "\n",
        "\n",
        "# Results:\n",
        "## Lags with 56 days shift (far away past) are not as important\n",
        "## as nearest past lags\n",
        "## and at some point will be just noise for our model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 30 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[239]\ttraining's rmse: 2.87311\tvalid_1's rmse: 2.41069\n",
            "Standart RMSE 2.410686921015837\n",
            "release 0.0\n",
            "sell_price 0.018\n",
            "price_max 0.0038\n",
            "price_min 0.0025\n",
            "price_std 0.0081\n",
            "price_mean 0.0072\n",
            "price_norm 0.0488\n",
            "price_nunique 0.0177\n",
            "item_nunique 0.0054\n",
            "price_momentum -0.0001\n",
            "price_momentum_m 0.0292\n",
            "price_momentum_y 0.0099\n",
            "tm_d 0.0039\n",
            "tm_w 0.0009\n",
            "tm_m -0.0003\n",
            "tm_y 0.0\n",
            "tm_wm 0.0003\n",
            "tm_dw 0.1172\n",
            "tm_w_end 0.0107\n",
            "sales_lag_56 0.0227\n",
            "sales_lag_57 0.0148\n",
            "sales_lag_58 0.0037\n",
            "sales_lag_59 0.004\n",
            "sales_lag_60 -0.0006\n",
            "sales_lag_61 0.0045\n",
            "sales_lag_62 0.0082\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55_sl3Fepr4L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "895eb281-1cf5-48c4-b8ec-9a2879d3efc5"
      },
      "source": [
        "########################### PCA\n",
        "#################################################################################\n",
        "\n",
        "# The main question here - can we have \n",
        "# almost same rmse boost with less features\n",
        "# less dimensionality?\n",
        "\n",
        "# Lets try PCA and make 7->3 dimensionality reduction\n",
        "\n",
        "# PCA is \"unsupervised\" learning\n",
        "# and with shifted target we can be sure\n",
        "# that we have no Target leakage\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def make_pca(df, pca_col, n_days):\n",
        "    print('PCA:', pca_col, n_days)\n",
        "    \n",
        "    # We don't need any other columns to make pca\n",
        "    pca_df = df[[pca_col,'d',TARGET]]\n",
        "    \n",
        "    # If we are doing pca for other series \"levels\" \n",
        "    # we need to agg first\n",
        "    if pca_col != 'id':\n",
        "        merge_base = pca_df[[pca_col,'d']]\n",
        "        pca_df = pca_df.groupby([pca_col,'d'])[TARGET].agg(['sum']).reset_index()\n",
        "        pca_df[TARGET] = pca_df['sum']\n",
        "        del pca_df['sum']\n",
        "    \n",
        "    # Min/Max scaling\n",
        "    pca_df[TARGET] = pca_df[TARGET]/pca_df[TARGET].max()\n",
        "    \n",
        "    # Making \"lag\" in old way (not parallel)\n",
        "    LAG_DAYS = [col for col in range(1,n_days+1)]\n",
        "    format_s = '{}_pca_'+pca_col+str(n_days)+'_{}'\n",
        "    pca_df = pca_df.assign(**{\n",
        "            format_s.format(col, l): pca_df.groupby([pca_col])[col].transform(lambda x: x.shift(l))\n",
        "            for l in LAG_DAYS\n",
        "            for col in [TARGET]\n",
        "        })\n",
        "    \n",
        "    pca_columns = list(pca_df)[3:]\n",
        "    pca_df[pca_columns] = pca_df[pca_columns].fillna(0)\n",
        "    pca = PCA(random_state=SEED)\n",
        "    \n",
        "    # You can use fit_transform here\n",
        "    pca.fit(pca_df[pca_columns])\n",
        "    pca_df[pca_columns] = pca.transform(pca_df[pca_columns])\n",
        "    \n",
        "    print(pca.explained_variance_ratio_)\n",
        "    \n",
        "    # we will keep only 3 most \"valuable\" columns/dimensions \n",
        "    keep_cols = pca_columns[:3]\n",
        "    print('Columns to keep:', keep_cols)\n",
        "    \n",
        "    # If we are doing pca for other series \"levels\"\n",
        "    # we need merge back our results to merge_base df\n",
        "    # and only than return resulted df\n",
        "    # I'll skip that step here\n",
        "    \n",
        "    return pca_df[keep_cols]\n",
        "\n",
        "\n",
        "# Make PCA\n",
        "grid_df = pd.concat([grid_df, make_pca(grid_df,'id',7)], axis=1)\n",
        "\n",
        "# Make features test\n",
        "test_model = make_fast_test(grid_df)\n",
        "\n",
        "# Remove test features\n",
        "# As we will compare performance with baseline model for now\n",
        "keep_cols = [col for col in list(grid_df) if '_pca_' not in col]\n",
        "grid_df = grid_df[keep_cols]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PCA: id 7\n",
            "[0.72224136 0.06621842 0.05938444 0.04201445 0.03891686 0.03614344\n",
            " 0.03508102]\n",
            "Columns to keep: ['sales_pca_id7_1', 'sales_pca_id7_2', 'sales_pca_id7_3']\n",
            "Training until validation scores don't improve for 30 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[339]\ttraining's rmse: 2.62621\tvalid_1's rmse: 2.26252\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qy6SxpDpuVD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "a3550551-0a22-4bf2-c981-2db9ffea5793"
      },
      "source": [
        "########################### Mean/std target encoding\n",
        "#################################################################################\n",
        "\n",
        "# We will use these three columns for test\n",
        "# (in combination with store_id)\n",
        "icols = ['item_id','cat_id','dept_id']\n",
        "\n",
        "# But we can use any other column or even multiple groups\n",
        "# like these ones\n",
        "#            'state_id',\n",
        "#            'store_id',\n",
        "#            'cat_id',\n",
        "#            'dept_id',\n",
        "#            ['state_id', 'cat_id'],\n",
        "#            ['state_id', 'dept_id'],\n",
        "#            ['store_id', 'cat_id'],\n",
        "#            ['store_id', 'dept_id'],\n",
        "#            'item_id',\n",
        "#            ['item_id', 'state_id'],\n",
        "#            ['item_id', 'store_id']\n",
        "\n",
        "# There are several ways to do \"mean\" encoding\n",
        "## K-fold scheme\n",
        "## LOO (leave one out)\n",
        "## Smoothed/regularized \n",
        "## Expanding mean\n",
        "## etc \n",
        "\n",
        "# You can test as many options as you want\n",
        "# and decide what to use\n",
        "# Because of memory issues you can't \n",
        "# use many features.\n",
        "\n",
        "# We will use simple target encoding\n",
        "# by std and mean agg\n",
        "for col in icols:\n",
        "    print('Encoding', col)\n",
        "    temp_df = grid_df[grid_df['d']<=(1913-28)] # to be sure we don't have leakage in our validation set\n",
        "    \n",
        "    temp_df = temp_df.groupby([col,'store_id']).agg({TARGET: ['std','mean']})\n",
        "    joiner = '_'+col+'_encoding_'\n",
        "    temp_df.columns = [joiner.join(col).strip() for col in temp_df.columns.values]\n",
        "    temp_df = temp_df.reset_index()\n",
        "    grid_df = grid_df.merge(temp_df, on=[col,'store_id'], how='left')\n",
        "    del temp_df\n",
        "\n",
        "# Make features test\n",
        "test_model = make_fast_test(grid_df)\n",
        "\n",
        "# Remove test features\n",
        "keep_cols = [col for col in list(grid_df) if '_encoding_' not in col]\n",
        "grid_df = grid_df[keep_cols]\n",
        "\n",
        "# Bad thing that for some items  \n",
        "# we are using past and future values.\n",
        "# But we are looking for \"categorical\" similiarity\n",
        "# on a \"long run\". So future here is not a big problem."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoding item_id\n",
            "Encoding cat_id\n",
            "Encoding dept_id\n",
            "Training until validation scores don't improve for 30 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[400]\ttraining's rmse: 2.77636\tvalid_1's rmse: 2.39284\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApJh0YVupxgC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "6d7356cb-3e6f-4a15-9f61-e68a060c2689"
      },
      "source": [
        "########################### Last non O sale\n",
        "#################################################################################\n",
        "\n",
        "def find_last_sale(df,n_day):\n",
        "    \n",
        "    # Limit initial df\n",
        "    ls_df = df[['id','d',TARGET]]\n",
        "    \n",
        "    # Convert target to binary\n",
        "    ls_df['non_zero'] = (ls_df[TARGET]>0).astype(np.int8)\n",
        "    \n",
        "    # Make lags to prevent any leakage\n",
        "    ls_df['non_zero_lag'] = ls_df.groupby(['id'])['non_zero'].transform(lambda x: x.shift(n_day).rolling(2000,1).sum()).fillna(-1)\n",
        "\n",
        "    temp_df = ls_df[['id','d','non_zero_lag']].drop_duplicates(subset=['id','non_zero_lag'])\n",
        "    temp_df.columns = ['id','d_min','non_zero_lag']\n",
        "\n",
        "    ls_df = ls_df.merge(temp_df, on=['id','non_zero_lag'], how='left')\n",
        "    ls_df['last_sale'] = ls_df['d'] - ls_df['d_min']\n",
        "\n",
        "    return ls_df[['last_sale']]\n",
        "\n",
        "\n",
        "# Find last non zero\n",
        "# Need some \"dances\" to fit in memory limit with groupers\n",
        "grid_df = pd.concat([grid_df, find_last_sale(grid_df,1)], axis=1)\n",
        "\n",
        "# Make features test\n",
        "test_model = make_fast_test(grid_df)\n",
        "\n",
        "# Remove test features\n",
        "keep_cols = [col for col in list(grid_df) if 'last_sale' not in col]\n",
        "grid_df = grid_df[keep_cols]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 30 rounds.\n",
            "Early stopping, best iteration is:\n",
            "[349]\ttraining's rmse: 2.6723\tvalid_1's rmse: 2.29713\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKGu6Urvpz1y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "fb1c9852-b8d0-4d79-bb59-79c252825b2f"
      },
      "source": [
        "########################### Apply on grid_df\n",
        "#################################################################################\n",
        "# lets read grid from \n",
        "# https://www.kaggle.com/kyakovlev/m5-simple-fe\n",
        "# to be sure that our grids are aligned by index\n",
        "grid_df = pd.read_pickle('grid_part_1.pkl')\n",
        "grid_df[TARGET][grid_df['d']>(1913-28)] = np.nan\n",
        "base_cols = list(grid_df)\n",
        "\n",
        "icols =  [\n",
        "            ['state_id'],\n",
        "            ['store_id'],\n",
        "            ['cat_id'],\n",
        "            ['dept_id'],\n",
        "            ['state_id', 'cat_id'],\n",
        "            ['state_id', 'dept_id'],\n",
        "            ['store_id', 'cat_id'],\n",
        "            ['store_id', 'dept_id'],\n",
        "            ['item_id'],\n",
        "            ['item_id', 'state_id'],\n",
        "            ['item_id', 'store_id']\n",
        "            ]\n",
        "\n",
        "for col in icols:\n",
        "    print('Encoding', col)\n",
        "    col_name = '_'+'_'.join(col)+'_'\n",
        "    grid_df['enc'+col_name+'mean'] = grid_df.groupby(col)[TARGET].transform('mean').astype(np.float16)\n",
        "    grid_df['enc'+col_name+'std'] = grid_df.groupby(col)[TARGET].transform('std').astype(np.float16)\n",
        "\n",
        "keep_cols = [col for col in list(grid_df) if col not in base_cols]\n",
        "grid_df = grid_df[['id','d']+keep_cols]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoding ['state_id']\n",
            "Encoding ['store_id']\n",
            "Encoding ['cat_id']\n",
            "Encoding ['dept_id']\n",
            "Encoding ['state_id', 'cat_id']\n",
            "Encoding ['state_id', 'dept_id']\n",
            "Encoding ['store_id', 'cat_id']\n",
            "Encoding ['store_id', 'dept_id']\n",
            "Encoding ['item_id']\n",
            "Encoding ['item_id', 'state_id']\n",
            "Encoding ['item_id', 'store_id']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQEsqWW0p2OS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a35909d9-97db-484f-db29-3f983cae67ec"
      },
      "source": [
        "#################################################################################\n",
        "print('Save Mean/Std encoding')\n",
        "grid_df.to_pickle('mean_encoding_df.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Save Mean/Std encoding\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsbOlHx0p4Ao",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        },
        "outputId": "1fbc5ff0-85b7-47ac-b6b2-e021e9c6208d"
      },
      "source": [
        "########################### Final list of new features\n",
        "#################################################################################\n",
        "grid_df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 47735397 entries, 0 to 47735396\n",
            "Data columns (total 24 columns):\n",
            " #   Column                     Dtype   \n",
            "---  ------                     -----   \n",
            " 0   id                         category\n",
            " 1   d                          int16   \n",
            " 2   enc_state_id_mean          float16 \n",
            " 3   enc_state_id_std           float16 \n",
            " 4   enc_store_id_mean          float16 \n",
            " 5   enc_store_id_std           float16 \n",
            " 6   enc_cat_id_mean            float16 \n",
            " 7   enc_cat_id_std             float16 \n",
            " 8   enc_dept_id_mean           float16 \n",
            " 9   enc_dept_id_std            float16 \n",
            " 10  enc_state_id_cat_id_mean   float16 \n",
            " 11  enc_state_id_cat_id_std    float16 \n",
            " 12  enc_state_id_dept_id_mean  float16 \n",
            " 13  enc_state_id_dept_id_std   float16 \n",
            " 14  enc_store_id_cat_id_mean   float16 \n",
            " 15  enc_store_id_cat_id_std    float16 \n",
            " 16  enc_store_id_dept_id_mean  float16 \n",
            " 17  enc_store_id_dept_id_std   float16 \n",
            " 18  enc_item_id_mean           float16 \n",
            " 19  enc_item_id_std            float16 \n",
            " 20  enc_item_id_state_id_mean  float16 \n",
            " 21  enc_item_id_state_id_std   float16 \n",
            " 22  enc_item_id_store_id_mean  float16 \n",
            " 23  enc_item_id_store_id_std   float16 \n",
            "dtypes: category(1), float16(22), int16(1)\n",
            "memory usage: 2.1 GB\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}

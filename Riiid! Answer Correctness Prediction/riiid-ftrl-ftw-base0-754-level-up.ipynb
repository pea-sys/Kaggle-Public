{"cells":[{"metadata":{},"cell_type":"markdown","source":"datatablepackageを使用したFTRLモデル   \n序盤に作成したノートブック  \n最終的には使用してません  \n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":false,"_kg_hide-output":true},"cell_type":"code","source":"# installing datatable\n!pip install ../input/python-datatable/datatable-0.11.0-cp37-cp37m-manylinux2010_x86_64.whl","execution_count":1,"outputs":[{"output_type":"stream","text":"Processing /kaggle/input/python-datatable/datatable-0.11.0-cp37-cp37m-manylinux2010_x86_64.whl\nInstalling collected packages: datatable\nSuccessfully installed datatable-0.11.0\n\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.3.3 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"## importing packages\nimport datatable as dt\nimport pandas as pd\nimport numpy as np\nfrom datatable.models import Ftrl\nfrom datatable import (dt, f, by, ifelse, update, sort, count, min, max, mean, sum, rowsum)\nfrom sklearn.metrics import roc_auc_score\nimport datetime\n\nimport riiideducation","execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"TARGET = 'answered_correctly'\nDEBUG = True\nmap_prior = {True:1, False:0}\nTRAIN_SIZE = 90000000","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading data\nUsing the .jay format of the training data is the best option for datatable. It is available as a **Kaggle Dataset** [here](https://www.kaggle.com/rohanrao/riiid-train-data-multiple-formats).\n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"## reading data\ntrain = dt.fread(\"../input/riiid-train-data-multiple-formats/riiid_train.jay\")\nquestions = dt.fread(\"../input/riiid-test-answer-prediction/questions.csv\")","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## viewing train data\n#train= train[:,[\"timestamp\",\"user_id\",\"content_id\",\"content_type_id\",\"task_container_id\",\"user_answer\",\"answered_correctly\",\"prior_question_elapsed_time\",\"prior_question_had_explanation\"]]#\n## merging questions metadata with train data\nquestions.key = \"question_id\"\ntrain.names = {\"content_id\": \"question_id\"}\n\ntrain = train[dt.f.content_type_id == 0, :]\ntrain = train[:, :, dt.join(questions)]\ntrain['prior_question_elapsed_time'] = dt.f.prior_question_elapsed_time / 1000\ntrain['timestamp'] = dt.f.timestamp / 1000\ndel train['row_id'], train['content_type_id'], train['user_answer'], train['correct_answer']","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engeering"},{"metadata":{"trusted":true},"cell_type":"code","source":"times_stack = dt.Frame()\ntimes_stack_features = ['user_id','timestamp','prior_question_elapsed_time', 'prior_question_had_explanation']\ntimes_m_gl = dt.Frame()\nintervals_m_gl = dt.Frame()\n\ndef get_shift_features():\n    task_m = times_stack[0, :, by('user_id','timestamp')]\n    task_m[:, update(shift_timestamp = dt.shift(f.timestamp),n=1), by(\"user_id\")]\n    task_m = task_m[:, :, sort(-f.user_id, -f.timestamp)]\n    task_m[:, update(prior_question_elapsed_time = dt.shift(f.prior_question_elapsed_time),n=1), by(\"user_id\")]\n    task_m[:, update(prior_question_had_explanation = dt.shift(f.prior_question_had_explanation),n=1), by(\"user_id\")]\n    #task_m = task_m[:, :, sort(f.user_id, f.timestamp)]\n    task_m['interval'] = f.timestamp - f.shift_timestamp\n    task_m = task_m[:,['user_id','timestamp','interval','prior_question_elapsed_time','prior_question_had_explanation']]\n    task_m.key= ([\"user_id\",\"timestamp\"])\n    \n    interval_m = task_m[:, {'interval_mean':mean(f.interval)}, by('user_id')]\n    interval_m.key= ([\"user_id\"])\n    return task_m, interval_m\n\n\nquestions_stack = dt.Frame()\nquestions_stack_features = ['question_id','part', TARGET,'prior_question_elapsed_time','user_id']\nquestion_m_gl = dt.Frame()\npart_m_gl = dt.Frame()\ndef get_question_features():\n    question_m = questions_stack[:, {'question_mean':mean(f.answered_correctly),\n                                     'elapsed_time_mean':mean(f.prior_question_elapsed_time)}, by('question_id')]\n    part_m = questions_stack[:, {'part_time_mean':mean(f.prior_question_elapsed_time)}, by('part','user_id')]\n    \n    question_m.key= ([\"question_id\"])\n    part_m.key= ([\"part\",'user_id'])\n    return question_m, part_m\n\n\nusers_stack = dt.Frame()\nusers_stack_features = ['user_id',TARGET,'elapsed_time_per']\nusers_m_gl = dt.Frame()\n\ndef get_user_features():\n    user_m = users_stack[:, {'user_mean':mean(f.answered_correctly),\n                             'user_count':count(f.answered_correctly),\n                             'elapsed_time_per_mean':mean(f.elapsed_time_per)}, by('user_id')]\n    user_m.key= ([\"user_id\"])\n    return user_m\n             ","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape)\n#TRAIN_SIZE = int(len(train) / 9)\nprint(train.shape)","execution_count":7,"outputs":[{"output_type":"stream","text":"(99271300, 10)\n(99271300, 10)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create TrainigSet\ndt_now = datetime.datetime.now()\n\nX_train = train[:TRAIN_SIZE, :]\n\ntimes_stack.rbind(X_train[:,times_stack_features])\ntimes_m_gl,intervals_m_gl = get_shift_features()\ndel X_train['prior_question_elapsed_time'], X_train['prior_question_had_explanation']\nX_train = X_train[:, :, dt.join(times_m_gl)]\nX_train = X_train[:, :, dt.join(intervals_m_gl)]\n\nquestions_stack.rbind(X_train[:,questions_stack_features])\nquestion_m_gl, part_m_gl = get_question_features()\nX_train = X_train[:, :, dt.join(question_m_gl)]\nX_train = X_train[:, :, dt.join(part_m_gl)]\nX_train['elapsed_time_per'] = f.prior_question_elapsed_time / f.elapsed_time_mean\n\nusers_stack.rbind(X_train[:,users_stack_features])\nuser_m_gl = get_user_features()\nX_train = X_train[:, :, dt.join(user_m_gl)]\ny_train = X_train[:,TARGET]\nprint(datetime.datetime.now() - dt_now)","execution_count":8,"outputs":[{"output_type":"stream","text":"0:03:03.475692\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create ValidationSet\nX_valid = train[TRAIN_SIZE:, :]\n\ntimes_stack.rbind(X_valid[:,times_stack_features])\ntimes_m_gl,intervals_m_gl = get_shift_features()\ndel X_valid['prior_question_elapsed_time'], X_valid['prior_question_had_explanation']\nX_valid = X_valid[:, :, dt.join(times_m_gl)]\nX_valid = X_valid[:, :, dt.join(intervals_m_gl)]\n\nquestions_stack.rbind(X_valid[:,questions_stack_features])\nquestion_m_gl,part_m_gl = get_question_features()\nX_valid = X_valid[:, :, dt.join(question_m_gl)]\nX_valid = X_valid[:, :, dt.join(part_m_gl)]\nX_valid['elapsed_time_per'] = f.prior_question_elapsed_time / f.elapsed_time_mean\n\nusers_stack.rbind(X_valid[:,users_stack_features])\nuser_m_gl = get_user_features()\nX_valid = X_valid[:, :, dt.join(user_m_gl)]\ny_valid = X_valid[:,TARGET]","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = [\"user_id\", \"question_id\", \"prior_question_elapsed_time\"]\nquestion_features = [\"bundle_id\", \"part\"]#,'interval',\"elapsed_time_per\", \"tags\"\ncreate_features = [\"user_mean\", \"question_mean\",\"elapsed_time_mean\",'elapsed_time_per_mean','prior_question_had_explanation','part_time_mean']\n\nX_train = X_train[:, train_features + question_features + create_features]\nX_valid = X_valid[:, train_features + question_features + create_features]","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## building and validating FTRL model 0.005以上上がらない特徴は弱い\n#Base Validation AUC: 0.7604151853254311\n#part_time_mean Validation AUC: 0.7614426241046937\n#timestamp 0 only f.timestamp > 1000 only Validation AUC: 0.5131733347825258\nmodel_ftrl = Ftrl()\nmodel_ftrl.interactions = [[\"question_id\",\"part\"],[\"question_id\",\"bundle_id\"]]\nmodel_ftrl.fit(X_train, y_train, X_validation=X_valid, y_validation=y_valid)\ny_pred = model_ftrl.predict(X_valid)\n    \nprint(f\"Validation AUC: {roc_auc_score(y_valid.to_numpy(), y_pred.to_numpy())}\")\ndel X_train, y_train, X_valid, y_valid, y_pred","execution_count":11,"outputs":[{"output_type":"stream","text":"Validation AUC: 0.7613873374174995\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train['prior_question_elapsed_time'], train['prior_question_had_explanation']\ntrain = train[:, :, dt.join(times_m_gl)]\ntrain = train[:, :, dt.join(intervals_m_gl)]\n\ntrain = train[:, :, dt.join(question_m_gl)]\ntrain = train[:, :, dt.join(part_m_gl)]\ntrain['elapsed_time_per'] = f.prior_question_elapsed_time / f.elapsed_time_mean\ntrain = train[:, :, dt.join(user_m_gl)]","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_ftrl.reset()\nmodel_ftrl.fit(train[:, train_features + question_features + create_features], train[:,TARGET])\ndel train\nmodel_ftrl.feature_importances[:, :, sort(f.feature_importance)]","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"<Frame#7f221447d900 13x2>","text/html":"<div class='datatable'>\n  <table class='frame'>\n  <thead>\n    <tr class='colnames'><td class='row_index'></td><th>feature_name</th><th>feature_importance</th></tr>\n    <tr class='coltypes'><td class='row_index'></td><td class='str' title='str32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td><td class='float' title='float32'>&#x25AA;&#x25AA;&#x25AA;&#x25AA;</td></tr>\n  </thead>\n  <tbody>\n    <tr><td class='row_index'>0</td><td>elapsed_time_per_mean</td><td>0.0289361</td></tr>\n    <tr><td class='row_index'>1</td><td>user_id</td><td>0.0445998</td></tr>\n    <tr><td class='row_index'>2</td><td>elapsed_time_mean</td><td>0.046575</td></tr>\n    <tr><td class='row_index'>3</td><td>part_time_mean</td><td>0.0484037</td></tr>\n    <tr><td class='row_index'>4</td><td>bundle_id</td><td>0.0513587</td></tr>\n    <tr><td class='row_index'>5</td><td>question_id:bundle_id</td><td>0.0547996</td></tr>\n    <tr><td class='row_index'>6</td><td>question_id</td><td>0.0582589</td></tr>\n    <tr><td class='row_index'>7</td><td>question_id:part</td><td>0.0585604</td></tr>\n    <tr><td class='row_index'>8</td><td>question_mean</td><td>0.157969</td></tr>\n    <tr><td class='row_index'>9</td><td>prior_question_elapsed_time</td><td>0.176467</td></tr>\n    <tr><td class='row_index'>10</td><td>user_mean</td><td>0.207413</td></tr>\n    <tr><td class='row_index'>11</td><td>part</td><td>0.5</td></tr>\n    <tr><td class='row_index'>12</td><td>prior_question_had_explanation</td><td>1</td></tr>\n  </tbody>\n  </table>\n  <div class='footer'>\n    <div class='frame_dimensions'>13 rows &times; 2 columns</div>\n  </div>\n</div>\n"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"## initializing test environment\nenv = riiideducation.make_env()\niter_test = env.iter_test()","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## inferencing and incremental learning\nrefresh_interval = -1\nprev_test = pd.DataFrame()\nfor (current_test, current_prediction_df) in iter_test:\n    current_test['prior_question_had_explanation'].fillna(False,inplace=True)\n    current_test['prior_question_had_explanation'] = current_test['prior_question_had_explanation'].map(map_prior).astype(np.int8)\n        \n    # extracting previous batch's targets\n    prev_target = eval(current_test[\"prior_group_answers_correct\"].iloc[0])\n\n    # incremental learning of FTRL model\n    if (prev_test.shape[0] > 0):\n        prev_test[TARGET] = np.array(prev_target)\n        ## viewing train data\n        prev_test = prev_test[dt.f.content_type_id == 0, :]\n        del prev_test[\"content_type_id\"]\n\n        questions_stack.rbind(prev_test[:,questions_stack_features])\n        users_stack.rbind(prev_test[:,users_stack_features])\n\n        if refresh_interval < 0:\n            question_m_gl, part_m_gl = get_question_features()\n            user_m_gl = get_user_features()\n            print('feature refresh')\n            refresh_interval = 50000\n\n        y_prev_test = prev_test[:, TARGET]\n        X_prev_test = prev_test[:, train_features + question_features + create_features]\n        model_ftrl.fit(X_prev_test, y_prev_test)\n\n    # inferencing of current batch\n    X_test = dt.Frame(current_test)\n    ## merging questions metadata with train data\n    X_test.names = {\"content_id\": \"question_id\"}\n    X_test = X_test[:, :, dt.join(questions)]\n    X_test['prior_question_elapsed_time'] = dt.f.prior_question_elapsed_time / 1000\n    X_test['timestamp'] = dt.f.timestamp / 1000\n\n    # time_stack target no use\n    refresh_interval -= prev_test.shape[0]\n    times_stack.rbind(X_test[:,times_stack_features])\n    if refresh_interval < 0: #user_idを渡せば高速化出来るが、使いまわしは出来ない（時間内に収まるならuser_id指定が良い)\n        times_m_gl,intervals_m_gl = get_shift_features()\n\n    del X_test['prior_question_elapsed_time'], X_test['prior_question_had_explanation']\n    X_test = X_test[:, :, dt.join(times_m_gl)]\n    X_test = X_test[:, :, dt.join(intervals_m_gl)]\n    X_test = X_test[:, :, dt.join(question_m_gl)]\n    X_test = X_test[:, :, dt.join(part_m_gl)]\n    \n    X_test['elapsed_time_per'] = f.prior_question_elapsed_time / f.elapsed_time_mean\n    \n    X_test = X_test[:, :, dt.join(user_m_gl)]\n    X_test[f.user_mean == None,'user_mean'] = 0.65\n    X_test[f.question_mean == None,'question_mean'] = 0.65\n\n    # retaining current batch data for next batch\n    prev_test = X_test.copy(deep = True)\n    \n    current_prediction_df.answered_correctly = model_ftrl.predict(X_test[dt.f.content_type_id == 0, :][:, train_features + question_features + create_features]).to_numpy().ravel()\n    env.predict(current_prediction_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}